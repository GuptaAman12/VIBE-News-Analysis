{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf708f7",
   "metadata": {},
   "source": [
    "# VIBE — Headline Frame Game (Final)\n",
    "\n",
    "Full pipeline + classifier stub.\n",
    "\n",
    "Run cells top-to-bottom. Install packages in the first cell if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install --quiet feedparser requests diskcache beautifulsoup4 nltk spacy vaderSentiment scikit-learn matplotlib wordcloud ipywidgets plotly\n",
    "# Optional heavy packages for HF & BERTopic (uncomment if you want these features):\n",
    "# !pip install --quiet transformers sentence-transformers umap-learn hdbscan bertopic\n",
    "# After installing spaCy model run:\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, json, pickle, hashlib, logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "# spaCy (optional)\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception as e:\n",
    "    print('spaCy not available or model missing:', e)\n",
    "    nlp = None\n",
    "\n",
    "# VADER sentiment\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    vader = None\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Optional heavy libraries (loaded later if installed)\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    HF_AVAILABLE = True\n",
    "except Exception:\n",
    "    HF_AVAILABLE = False\n",
    "    pipeline = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from bertopic import BERTopic\n",
    "    BERTOPIC_AVAILABLE = True\n",
    "except Exception:\n",
    "    BERTOPIC_AVAILABLE = False\n",
    "    SentenceTransformer = None\n",
    "    BERTopic = None\n",
    "\n",
    "# Visualization & widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('VIBE_Final')\n",
    "\n",
    "CACHE_DIR = './vibe_cache'\n",
    "CACHE_EXPIRE_HOURS = 6\n",
    "\n",
    "DEFAULT_RSS_FEEDS = {\n",
    "    'NDTV': 'https://feeds.feedburner.com/ndtvnews-latest',\n",
    "    'TheTimesOfIndia': 'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',\n",
    "    'HindustanTimes': 'https://www.hindustantimes.com/feeds/rss/topnews/rssfeed.xml',\n",
    "}\n",
    "\n",
    "FRAME_KEYWORDS = {\n",
    "    'conflict': ['clash', 'attack', 'slams', 'condemns', 'fight', 'protest', 'violence', 'conflict'],\n",
    "    'human_interest': ['family', 'children', 'stories', 'personal', 'meet', 'recounts'],\n",
    "    'economic': ['economy', 'inflation', 'jobs', 'market', 'business', 'trade', 'GDP'],\n",
    "    'responsibility': ['responsible', 'fail', 'blame', 'accountability', 'investigate'],\n",
    "    'morality': ['moral', 'immoral', 'ethical', 'virtue', 'sin'],\n",
    "}\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching helpers\n",
    "def cache_get(key: str):\n",
    "    path = os.path.join(CACHE_DIR, hashlib.sha1(key.encode()).hexdigest() + '.pkl')\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            data, ts = pickle.load(open(path,'rb'))\n",
    "            if datetime.utcnow() - ts < timedelta(hours=CACHE_EXPIRE_HOURS):\n",
    "                return data\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def cache_set(key: str, value: Any):\n",
    "    path = os.path.join(CACHE_DIR, hashlib.sha1(key.encode()).hexdigest() + '.pkl')\n",
    "    pickle.dump((value, datetime.utcnow()), open(path,'wb'))\n",
    "\n",
    "# RSS fetching\n",
    "def fetch_rss(url: str, retries: int = 2, backoff: float = 1.0) -> Any:\n",
    "    key = f\"rss::{url}\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        logger.info(f\"Cache hit for {url}\")\n",
    "        return cached\n",
    "    last_exc = None\n",
    "    for i in range(retries+1):\n",
    "        try:\n",
    "            logger.info(f\"Fetching {url} (attempt {i+1})\")\n",
    "            feed = feedparser.parse(url)\n",
    "            cache_set(key, feed)\n",
    "            return feed\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            logger.warning(f\"Error fetching {url}: {e}\")\n",
    "            time.sleep(backoff * (2**i))\n",
    "    raise last_exc\n",
    "\n",
    "def fetch_all_feeds(feed_dict: Dict[str,str]) -> Dict[str, List[Dict]]:\n",
    "    results = {}\n",
    "    health = {}\n",
    "    for name, url in feed_dict.items():\n",
    "        try:\n",
    "            feed = fetch_rss(url)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                title = e.get('title','')\n",
    "                link = e.get('link','')\n",
    "                published = e.get('published', e.get('updated',''))\n",
    "                summary = e.get('summary','')\n",
    "                entries.append({'title': title, 'link': link, 'published': published, 'summary': summary, 'source': name})\n",
    "            results[name] = entries\n",
    "            health[name] = {'ok': True, 'count': len(entries)}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Feed {name} failed: {e}\")\n",
    "            results[name] = []\n",
    "            health[name] = {'ok': False, 'error': str(e)}\n",
    "    cache_set('feed_health', {'timestamp': datetime.utcnow().isoformat(), 'health': health})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb901ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning, dedupe, lemmatize\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return ''\n",
    "    t = unicodedata.normalize('NFKD', t)\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip()\n",
    "\n",
    "def clean_html(text: str) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return normalize_text(soup.get_text(separator=' '))\n",
    "\n",
    "def dedupe_entries(all_entries: List[Dict], threshold: float = 0.92) -> List[Dict]:\n",
    "    for e in all_entries:\n",
    "        e['title_norm'] = normalize_text(e.get('title','')).lower()\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for e in all_entries:\n",
    "        key = (e.get('link') or '') or e['title_norm']\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique.append(e)\n",
    "    if len(unique) > 1:\n",
    "        vec = TfidfVectorizer(stop_words='english', max_df=0.85)\n",
    "        X = vec.fit_transform([u['title_norm'] for u in unique])\n",
    "        sim = cosine_similarity(X)\n",
    "        to_drop = set()\n",
    "        for i in range(sim.shape[0]):\n",
    "            for j in range(i+1, sim.shape[1]):\n",
    "                if sim[i,j] > threshold:\n",
    "                    if len(unique[i]['title']) >= len(unique[j]['title']):\n",
    "                        to_drop.add(j)\n",
    "                    else:\n",
    "                        to_drop.add(i)\n",
    "        unique = [u for idx,u in enumerate(unique) if idx not in to_drop]\n",
    "    return unique\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    if nlp is None:\n",
    "        return text\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment, entities, frames\n",
    "def sentiment_vader(text: str) -> Dict:\n",
    "    if vader is None:\n",
    "        return {'neg': None, 'neu': None, 'pos': None, 'compound': None}\n",
    "    return vader.polarity_scores(text)\n",
    "\n",
    "def extract_entities(text: str) -> List[Tuple[str,str]]:\n",
    "    if nlp is None:\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "def detect_frames(text: str) -> Dict[str,int]:\n",
    "    t = (text or '').lower()\n",
    "    scores = {}\n",
    "    for f,kws in FRAME_KEYWORDS.items():\n",
    "        scores[f] = sum(1 for kw in kws if kw in t)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics (optionally using HF pipeline if passed)\n",
    "def compute_bias_metrics(entries: List[Dict], use_hf: bool=False, hf_pipeline=None, hf_batch_size: int=32):\n",
    "    texts = []\n",
    "    if use_hf and hf_pipeline is not None:\n",
    "        for e in entries:\n",
    "            texts.append((clean_html(e.get('title','')) + ' ' + clean_html(e.get('summary',''))).strip())\n",
    "        hf_results = []\n",
    "        for i in range(0, len(texts), hf_batch_size):\n",
    "            batch = texts[i:i+hf_batch_size]\n",
    "            try:\n",
    "                hf_results.extend(hf_pipeline(batch))\n",
    "            except Exception:\n",
    "                for t in batch:\n",
    "                    try:\n",
    "                        hf_results.extend(hf_pipeline(t[:512]))\n",
    "                    except Exception:\n",
    "                        hf_results.append({'label': None, 'score': None})\n",
    "    else:\n",
    "        hf_results = [None] * len(entries)\n",
    "\n",
    "    for idx, e in enumerate(entries):\n",
    "        e['title_clean'] = clean_html(e.get('title',''))\n",
    "        e['summary_clean'] = clean_html(e.get('summary',''))\n",
    "        e['title_lem'] = lemmatize_text(e['title_clean'])\n",
    "        e['summary_lem'] = lemmatize_text(e['summary_clean'])\n",
    "        e['sentiment_vader'] = sentiment_vader(e['title_clean'] + ' ' + e['summary_clean'])\n",
    "        e['sentiment_hf'] = hf_results[idx] if idx < len(hf_results) else None\n",
    "        e['entities'] = extract_entities(e['title_clean'] + ' ' + e['summary_clean'])\n",
    "        e['frames'] = detect_frames(e['title_clean'] + ' ' + e['summary_clean'])\n",
    "\n",
    "    agg = {}\n",
    "    for e in entries:\n",
    "        s = e['source']\n",
    "        if s not in agg:\n",
    "            agg[s] = {'count': 0, 'vader_compound_sum': 0.0, 'hf_scores': [], 'frames': {}, 'entities': {}}\n",
    "        agg[s]['count'] += 1\n",
    "        if isinstance(e['sentiment_vader'], dict) and e['sentiment_vader'].get('compound') is not None:\n",
    "            agg[s]['vader_compound_sum'] += e['sentiment_vader']['compound']\n",
    "        if e['sentiment_hf'] and isinstance(e['sentiment_hf'], dict):\n",
    "            lab = e['sentiment_hf'].get('label','')\n",
    "            sc = e['sentiment_hf'].get('score',0.0) or 0.0\n",
    "            lab_l = str(lab).lower()\n",
    "            if 'pos' in lab_l:\n",
    "                val = float(sc)\n",
    "            elif 'neg' in lab_l:\n",
    "                val = -float(sc)\n",
    "            else:\n",
    "                val = 0.0\n",
    "            agg[s]['hf_scores'].append(val)\n",
    "        for f,v in e['frames'].items():\n",
    "            agg[s]['frames'][f] = agg[s]['frames'].get(f,0) + v\n",
    "        for ent,_ in e['entities']:\n",
    "            agg[s]['entities'][ent] = agg[s]['entities'].get(ent,0) + 1\n",
    "\n",
    "    for s,v in agg.items():\n",
    "        v['avg_vader_compound'] = v['vader_compound_sum'] / max(1, v['count'])\n",
    "        v['avg_hf_score'] = (sum(v['hf_scores']) / len(v['hf_scores'])) if len(v['hf_scores'])>0 else None\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline runner\n",
    "def run_pipeline(feeds: Dict[str,str]=DEFAULT_RSS_FEEDS, use_hf: bool=False, hf_model: Optional[str]=None):\n",
    "    hf_pipe = None\n",
    "    if use_hf and HF_AVAILABLE and hf_model is not None:\n",
    "        try:\n",
    "            hf_pipe = pipeline('sentiment-analysis', model=hf_model, device=0 if __import__('torch').cuda.is_available() else -1, truncation=True)\n",
    "        except Exception as e:\n",
    "            print('Failed to init HF pipeline:', e)\n",
    "            hf_pipe = None\n",
    "    raw = fetch_all_feeds(feeds)\n",
    "    all_entries = []\n",
    "    for src, items in raw.items():\n",
    "        for it in items:\n",
    "            all_entries.append(it)\n",
    "    print(f'Fetched {len(all_entries)} articles')\n",
    "    unique = dedupe_entries(all_entries)\n",
    "    print(f'After dedupe: {len(unique)} articles')\n",
    "    agg = compute_bias_metrics(unique, use_hf=(use_hf and hf_pipe is not None), hf_pipeline=hf_pipe)\n",
    "    try:\n",
    "        plt.figure(figsize=(8,3))\n",
    "        sources = list(agg.keys())\n",
    "        vals = [agg[s]['avg_vader_compound'] for s in sources]\n",
    "        plt.bar(sources, vals)\n",
    "        plt.title('Avg VADER compound sentiment by source')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {'raw': raw, 'unique': unique, 'agg': agg}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47571e",
   "metadata": {},
   "source": [
    "## Frame Classifier — labeling UI + training\n",
    "\n",
    "Label headlines with frames, train a TF-IDF + LogisticRegression model, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling UI\n",
    "labeled_path = 'frame_labels.json'\n",
    "\n",
    "def save_labels(labels):\n",
    "    with open(labeled_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(labels, f, ensure_ascii=False, indent=2)\n",
    "    print('Saved', labeled_path)\n",
    "\n",
    "def load_labels():\n",
    "    if os.path.exists(labeled_path):\n",
    "        return json.load(open(labeled_path,'r',encoding='utf-8'))\n",
    "    return {}\n",
    "\n",
    "labels = load_labels()\n",
    "\n",
    "if 'results' not in globals():\n",
    "    print('Run the pipeline cell first: results = run_pipeline()')\n",
    "else:\n",
    "    unique = results['unique']\n",
    "    if len(unique) == 0:\n",
    "        print('No articles in results to label.')\n",
    "    else:\n",
    "        def show_item(i):\n",
    "            it = unique[i]\n",
    "            print(f\"Index: {i} | Source: {it.get('source')} | Published: {it.get('published')}\")\n",
    "            print(it.get('title'))\n",
    "            print('Summary:', it.get('summary'))\n",
    "            cur = labels.get(str(i), None)\n",
    "            print('Current label:', cur)\n",
    "\n",
    "        idx_widget = widgets.IntText(value=0, description='Index')\n",
    "        prev_btn = widgets.Button(description='Prev')\n",
    "        next_btn = widgets.Button(description='Next')\n",
    "        save_btn = widgets.Button(description='Save label')\n",
    "        label_dropdown = widgets.Dropdown(options=['none'] + list(FRAME_KEYWORDS.keys()), description='Label')\n",
    "        out = widgets.Output()\n",
    "\n",
    "        def update_display(change=None):\n",
    "            with out:\n",
    "                clear_output()\n",
    "                i = int(idx_widget.value)\n",
    "                if i < 0: idx_widget.value = 0\n",
    "                if i >= len(unique): idx_widget.value = len(unique)-1\n",
    "                show_item(idx_widget.value)\n",
    "                label_dropdown.value = labels.get(str(idx_widget.value), 'none')\n",
    "\n",
    "        def on_prev(b):\n",
    "            idx_widget.value = max(0, idx_widget.value-1)\n",
    "            update_display()\n",
    "        def on_next(b):\n",
    "            idx_widget.value = min(len(unique)-1, idx_widget.value+1)\n",
    "            update_display()\n",
    "        def on_save(b):\n",
    "            labels[str(idx_widget.value)] = label_dropdown.value\n",
    "            save_labels(labels)\n",
    "            print('Saved label for', idx_widget.value)\n",
    "\n",
    "        prev_btn.on_click(on_prev)\n",
    "        next_btn.on_click(on_next)\n",
    "        save_btn.on_click(on_save)\n",
    "\n",
    "        display(widgets.HBox([prev_btn, next_btn, idx_widget, label_dropdown, save_btn]))\n",
    "        display(out)\n",
    "        update_display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c12354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the TF-IDF + LogisticRegression classifier\n",
    "labels = load_labels()\n",
    "rows = []\n",
    "for k,v in labels.items():\n",
    "    idx = int(k)\n",
    "    it = results['unique'][idx]\n",
    "    text = (clean_html(it.get('title','')) + ' ' + clean_html(it.get('summary',''))).strip()\n",
    "    if v != 'none':\n",
    "        rows.append({'text': text, 'label': v})\n",
    "df_labels = pd.DataFrame(rows)\n",
    "print('Total labeled examples:', len(df_labels))\n",
    "if len(df_labels) < 10:\n",
    "    print('Label more examples before training (>=10 recommended).')\n",
    "else:\n",
    "    X = df_labels['text'].values\n",
    "    y = df_labels['label'].values\n",
    "    tf = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=1)\n",
    "    Xv = tf.fit_transform(X)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(Xv, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    yp = clf.predict(Xtest)\n",
    "    print('Accuracy:', accuracy_score(ytest, yp))\n",
    "    print(classification_report(ytest, yp))\n",
    "    pickle.dump({'vectorizer': tf, 'model': clf}, open('frame_classifier.pkl','wb'))\n",
    "    print('Saved frame_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate / Predict on all articles using trained classifier\n",
    "if os.path.exists('frame_classifier.pkl'):\n",
    "    obj = pickle.load(open('frame_classifier.pkl','rb'))\n",
    "    tf = obj['vectorizer']\n",
    "    clf = obj['model']\n",
    "    texts = [(clean_html(it.get('title','')) + ' ' + clean_html(it.get('summary',''))).strip() for it in results['unique']]\n",
    "    X = tf.transform(texts)\n",
    "    preds = clf.predict(X)\n",
    "    for it, p in zip(results['unique'], preds):\n",
    "        it['pred_frame'] = p\n",
    "    from collections import Counter\n",
    "    print('Predicted frame counts:', Counter(preds))\n",
    "else:\n",
    "    print('Train classifier first (run the training cell)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee5c17",
   "metadata": {},
   "source": [
    "## BERTopic (optional)\n",
    "\n",
    "Uncomment and run the BERTopic cell if you installed `bertopic` and `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic example (uncomment to run after installing heavy deps)\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from bertopic import BERTopic\n",
    "# sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# docs = [(it.get('title_clean','') + ' ' + it.get('summary_clean','')).strip() for it in results['unique']]\n",
    "# embeddings = sbert.encode(docs, show_progress_bar=True, convert_to_numpy=True)\n",
    "# topic_model = BERTopic(min_topic_size=8)\n",
    "# topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "# topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed results to file (including predictions if available)\n",
    "with open('vibe_results_full.json','w',encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print('Saved vibe_results_full.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f54cb",
   "metadata": {},
   "source": [
    "### Notebook created and saved to /mnt/data/VIBE_Headline_Frame_Game_Final.ipynb\n",
    "\n",
    "Run the pipeline cell, label data, train the classifier, and evaluate. After training, predictions will be attached to `results['unique']`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
