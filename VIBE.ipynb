{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: Setup and Installations (REVISED)\n",
        "# ==============================================================================\n",
        "# This cell installs all the necessary libraries and downloads the required\n",
        "# language models from NLTK and SpaCy.\n",
        "\n",
        "# --- Install Python packages ---\n",
        "!pip install newsapi-python wordcloud -q\n",
        "\n",
        "# --- Download SpaCy model ---\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "# --- Download NLTK models ---\n",
        "import nltk\n",
        "print(\"Starting download of required NLTK models...\")\n",
        "\n",
        "# ADDED 'punkt_tab' TO THIS LIST TO FIX THE LOOKUPERROR\n",
        "packages = ['stopwords', 'punkt', 'vader_lexicon', 'punkt_tab']\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        # Define search paths for different package types\n",
        "        if package == 'stopwords':\n",
        "            nltk.data.find(f'corpora/{package}')\n",
        "        elif package.startswith('punkt'): # Handles both 'punkt' and 'punkt_tab'\n",
        "            nltk.data.find(f'tokenizers/{package}')\n",
        "        else: # For vader_lexicon and others\n",
        "            nltk.data.find(f'sentiment/{package}.zip')\n",
        "        print(f\"‚úÖ Package '{package}' is already downloaded.\")\n",
        "    except LookupError:\n",
        "        print(f\"‚¨áÔ∏è  Downloading package '{package}'...\")\n",
        "        nltk.download(package, quiet=True)\n",
        "        print(f\"üëç Download of '{package}' complete.\")\n",
        "\n",
        "print(\"\\n‚úÖ All installations and downloads are complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoTHXDFiailo",
        "outputId": "48b548cf-b242-4856-8625-7e30be65e5db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/12.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.4/12.8 MB\u001b[0m \u001b[31m240.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m262.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Starting download of required NLTK models...\n",
            "‚úÖ Package 'stopwords' is already downloaded.\n",
            "‚úÖ Package 'punkt' is already downloaded.\n",
            "‚úÖ Package 'vader_lexicon' is already downloaded.\n",
            "‚úÖ Package 'punkt_tab' is already downloaded.\n",
            "\n",
            "‚úÖ All installations and downloads are complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: Import Libraries\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "import re\n",
        "from newsapi import NewsApiClient\n",
        "import os\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "hqjDcLYbcNY0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: Section 1 - Dataset Collection Function (NEW QUERY & ALL SOURCES)\n",
        "# ==============================================================================\n",
        "def collect_data():\n",
        "    \"\"\"\n",
        "    Fetches news on a specific topic from all available sources.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Section 1: Dataset Collection ---\")\n",
        "    api_key = 'e014b82fbd064b5d99a37739a6e2760a' # Your API Key\n",
        "    newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "    # 1. THE QUERY HAS BEEN UPDATED to be more specific.\n",
        "    # Using keywords and boolean operators gives the API the best instructions.\n",
        "    query = '\"pakistan\" AND \"india\"'\n",
        "\n",
        "    # The date range is still set to the last few days to get the \"latest\" news\n",
        "    today = datetime.now()\n",
        "    yesterday = today - timedelta(days=1)\n",
        "    three_days_ago = today - timedelta(days=6)\n",
        "    to_date = yesterday.strftime('%Y-%m-%d')\n",
        "    from_date = three_days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "    print(f\"Searching for topic '{query}' from {from_date} to {to_date} across all available sources.\")\n",
        "\n",
        "    all_articles = []\n",
        "    try:\n",
        "        articles = newsapi.get_everything(q=query,\n",
        "                                          from_param=from_date,\n",
        "                                          to=to_date,\n",
        "                                          language='en',\n",
        "                                          sort_by='publishedAt', # Sorts by newest first\n",
        "                                          page_size=100)\n",
        "        for article in articles['articles']:\n",
        "            source_name = article['source']['name'] if article['source'] else 'Unknown'\n",
        "            all_articles.append({ 'source': source_name, 'title': article['title'], 'description': article['description'], 'publishedAt': article['publishedAt'], 'url': article['url'] })\n",
        "    except Exception as e:\n",
        "        print(f\"Could not fetch articles. Error: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_articles)\n",
        "\n",
        "    # 2. THE FILTERING SECTION HAS BEEN REMOVED.\n",
        "    # The code now saves and returns the full DataFrame with all sources.\n",
        "    if not df.empty:\n",
        "        df.to_csv('raw_headlines.csv', index=False)\n",
        "        print(f\"\\nCollected {len(df)} articles from all sources and saved to raw_headlines.csv\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "CrUgBt4Jcf1y"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: Section 2 - Pre-processing Function\n",
        "# ==============================================================================\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Cleans and prepares the collected text data for analysis.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Section 2: Pre-processing ---\")\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(text):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "    def lemmatize_text(text):\n",
        "        doc = nlp(text)\n",
        "        return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "    df['text'] = df['title'].fillna('') + ' ' + df['description'].fillna('')\n",
        "    df['cleaned_text'] = df['text'].apply(clean_text).apply(remove_stopwords)\n",
        "    df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
        "\n",
        "    df.drop_duplicates(subset=['title', 'source'], inplace=True)\n",
        "    df = df[df['lemmatized_text'].str.strip() != '']\n",
        "\n",
        "    df.to_csv('clean_headlines.csv', index=False)\n",
        "    print(f\"Cleaned data has {len(df)} articles and is saved to clean_headlines.csv\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Kiac-wC_cqSS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 5: Section 3 - Analysis Function\n",
        "# ==============================================================================\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Performs sentiment analysis on the cleaned data.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Section 3: Analysis ---\")\n",
        "\n",
        "    # --- Sentiment Analysis ---\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    df['sentiment_scores'] = df['text'].apply(lambda text: sid.polarity_scores(text))\n",
        "    df = pd.concat([df.drop(['sentiment_scores'], axis=1), df['sentiment_scores'].apply(pd.Series)], axis=1)\n",
        "\n",
        "    avg_sentiment = df.groupby('source')[['neg', 'neu', 'pos', 'compound']].mean().reset_index()\n",
        "    print(\"\\n--- Average Sentiment per Outlet ---\")\n",
        "    print(avg_sentiment.to_string())\n",
        "\n",
        "    return df, avg_sentiment"
      ],
      "metadata": {
        "id": "sbxkzjoOcwEf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 6: Section 4 - Visualization Function\n",
        "# ==============================================================================\n",
        "def create_visualizations(df, avg_sentiment):\n",
        "    \"\"\"\n",
        "    Generates and saves word cloud visualizations for each news source.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Section 4: Visualization ---\")\n",
        "    if not os.path.exists('plots'):\n",
        "        os.makedirs('plots')\n",
        "\n",
        "    news_sources = df['source'].unique()\n",
        "\n",
        "    # 1. Word Clouds\n",
        "    for source in news_sources:\n",
        "        text = ' '.join(df[df['source'] == source]['lemmatized_text'])\n",
        "        if text:\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Word Cloud for {source}')\n",
        "            plt.savefig(f'plots/word_cloud_{source.replace(\" \", \"_\")}.png')\n",
        "            print(f\"Saved word cloud for {source}.\")\n",
        "            plt.close() # Close the plot to prevent it from displaying in the notebook output"
      ],
      "metadata": {
        "id": "zt-qDck8c8SG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: Main Execution Block\n",
        "# ==============================================================================\n",
        "# This final cell runs the entire pipeline from data collection to visualization.\n",
        "\n",
        "# Step 1: Collect Data\n",
        "raw_df = collect_data()\n",
        "\n",
        "# Step 2: Check if data was collected BEFORE proceeding\n",
        "if raw_df is not None and not raw_df.empty:\n",
        "\n",
        "    # Step 3: Pre-process Data\n",
        "    clean_df = preprocess_data(raw_df)\n",
        "\n",
        "    # Step 4: Analyze Data\n",
        "    analyzed_df, avg_sentiment = analyze_data(clean_df)\n",
        "\n",
        "    # Step 5: Create Visualizations\n",
        "    create_visualizations(analyzed_df, avg_sentiment)\n",
        "\n",
        "    print(\"\\n‚úÖ Project execution completed successfully!\")\n",
        "    print(\"\\nCheck the file browser on the left to find your CSV files and the 'plots' directory.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Project execution stopped because no articles were found. Please try a different search query or check your API key.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsxen-r-c_uZ",
        "outputId": "a5325715-630f-4460-dadf-e34c88a65b2e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Section 1: Dataset Collection ---\n",
            "Searching for topic '\"Mohsin Naqvi\" AND \"pakistan\" AND \"india\"' from 2025-09-30 to 2025-10-05 across all available sources.\n",
            "\n",
            "Collected 19 articles from all sources and saved to raw_headlines.csv\n",
            "\n",
            "--- Starting Section 2: Pre-processing ---\n",
            "Cleaned data has 19 articles and is saved to clean_headlines.csv\n",
            "\n",
            "--- Starting Section 3: Analysis ---\n",
            "\n",
            "--- Average Sentiment per Outlet ---\n",
            "                          source     neg     neu    pos  compound\n",
            "0                  ABC News (AU)  0.3330  0.6130  0.053  -0.92870\n",
            "1             Al Jazeera English  0.2155  0.7425  0.042  -0.69555\n",
            "2                       BBC News  0.1440  0.6860  0.170   0.27320\n",
            "3                   BusinessLine  0.1690  0.8310  0.000  -0.25840\n",
            "4                            CNA  0.0420  0.9010  0.057   0.20230\n",
            "5                   DW (English)  0.1440  0.8170  0.039  -0.75060\n",
            "6                 Foreign Policy  0.0820  0.8150  0.103   0.12800\n",
            "7            Gossiplankanews.com  0.0000  0.7980  0.202   0.88600\n",
            "8                 Ibtimes.com.au  0.3210  0.6790  0.000  -0.94420\n",
            "9   International Business Times  0.3210  0.6790  0.000  -0.94420\n",
            "10                Ndtvprofit.com  0.0690  0.8780  0.053  -0.26950\n",
            "11               The Japan Times  0.0970  0.7140  0.190   0.75430\n",
            "12            The Times of India  0.0706  0.7924  0.137   0.44946\n",
            "13           Yahoo Entertainment  0.0000  0.8460  0.154   0.84020\n",
            "\n",
            "--- Starting Section 4: Visualization ---\n",
            "Saved word cloud for The Times of India.\n",
            "Saved word cloud for ABC News (AU).\n",
            "Saved word cloud for International Business Times.\n",
            "Saved word cloud for Ibtimes.com.au.\n",
            "Saved word cloud for The Japan Times.\n",
            "Saved word cloud for Foreign Policy.\n",
            "Saved word cloud for Ndtvprofit.com.\n",
            "Saved word cloud for CNA.\n",
            "Saved word cloud for Yahoo Entertainment.\n",
            "Saved word cloud for BBC News.\n",
            "Saved word cloud for Al Jazeera English.\n",
            "Saved word cloud for BusinessLine.\n",
            "Saved word cloud for DW (English).\n",
            "Saved word cloud for Gossiplankanews.com.\n",
            "\n",
            "‚úÖ Project execution completed successfully!\n",
            "\n",
            "Check the file browser on the left to find your CSV files and the 'plots' directory.\n"
          ]
        }
      ]
    }
  ]
}